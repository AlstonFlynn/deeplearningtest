## 机器学习小杯理解

1. 人工智能的目标是,  让机器的行为看起来就像是人所表示出来的智能行为一样. 人工智能研究的主要领域有:  感知  (获取外部刺激信息并进行加工),  学习  (从样例和交互环境中学习),  认知  (自然语言处理,  推理,  规划决策).
2. 专家系统,  1970年,  等于知识库+推理机,  是一类具有专门知识和经验的计算机智能程序系统.
3. 让机器自动学习,  即是机器学习 Machine Learning. 机器学习的主要思路是:  设计学习算法,  让算法可以从数据  (数据等价于经验) 中自动分析并获取规律. 然后,  再利用获取到的规律对未知数据进行预测. 
4. 历史上关于人工智能打的最多的两个流派是①符号主义,  主张信息是符号,  人的认知过程等于符号的显式的逻辑运算. ②连接主义,  人的认知过程等于大量简单神经元构成的神经网络的信息处理过程. 上面说的神经网络就是连接主义模型.
5. 贡献度分配问题,  , 深度学习采用的模型一般比较复杂, 指样本的原始输入到输出目标之间的数据流经过多个线性或非线性的组件 (component)．因为每个组件都会对信息进行加工, 并进而影响后续的组件, 所以当我们最后得到输出结果时, 我们并不清楚其中每个组件的贡献是多少．这个问题叫作贡献度分配问题.
    1. 人工神经网络 ANN,  Artificial Neural Network ,   (简称-神经网络).
    2. 「组件」, 即 $y=f (x;\theta)$,  其中 x 输入,  y 输出, $\theta$ 是组件参数。f 可以是线性或非线性的变换。组件接受输入, 按照参数决定的某种规则进行变换, 然后输出结果, 这样的一个映射。
    3. 反向传播,  提供了一种梯度级别的贡献度分配机制. 「学习」是为了解决组件的贡献度分配问题.
6. 「学习」,  从机器学习的角度:  在多组件结构中解决每个组件的贡献度分配问题  (即每个组件的参数); 从统计角度说:  从有限样本中挑选一个假设 $f\in \mathcal{H}$,  使得这个假设在未知数据分布上的泛化误差尽可能小. H 是假设空间,  由模型结构确定.
    1. 我们给予一种确定的模型结构,  和有限的样本数据:  然后通过自动地调整模型数据,  使得某个预先设定的性能指标  (在期望意义下) 达到最优或者足够好的过程.
    2. 这个过程就是 $min_{\theta}\boldsymbol{E}_{ (x, y)\sim \mathcal{D}}[\mathcal{l} (f (x;\theta), y)]$. 其中
        1. $min_{\theta}$ 表示在模型参数的参数空间中选择的一个参数向量 $\theta$,  使得后面的目标函数取到最小值; 改变 $\theta$ 在数学上等价于“在参数空间中搜索”.
        2. $E[.]$ 表示期望. $ (x, y)\sim\mathcal{D}$ 表示 x 和 y 这对随机变量  (通常 x 是特征,  y 是标签),  服从 D 这个联合概率分布. 
        3. D 定义了: ①什么样的 x 更可能出现  (边缘概率),  ②给予确定的 x,  此时什么样的 y 更可能出现  (联合概率分布). 样本集合是从这个分布中抽样得到的结果.
        4. 真实的 D 分布不可能得到,  所以我们用大样本来做近似,  这个定理其实是「大数定理」. 也就是 $E_\mathcal{D}[\mathcal{l}]\approx \frac{1}{N}\sum_{i=1}^{N}\mathcal{l} (f (x_{i};\theta), y_{i})$. 当且仅当样本  (x,  y) 是独立同部分  (i.i.d). 这是从期望到平均值的退化.
        5. 隐含的关键假设是:  训练样本和未来测试数据来自同一个分布 $\mathcal{D}$. 否则泛化会失败.
    3. 学习的对象不是模型或规则,  而是参数.
    4. 性能指标用「损失函数」来度量,  用于衡量模型  (在某组参数下) 输出和真实标签之间的差距. 
7. 机器学习常见定理
    1. 用机器学习方法解决特定的问题,  需要模型; 而如何获得不同的模型? 靠经验调整,  多次试验,  调整训练集. 其中指导模型和学习算法设计的理论,  是「计算学习理论」,  其中最基础的理论是 PAC 学习理论  (可能近似正确). 
    2. 编码:  用一套确定的符号规则, 把信息表示为比特串  (onehot)
    3. 编码长度最小: 一个好的模型能够以较短的*模型描述*加上较小的*残差描述*来复现数据集, 因此使得总体编码长度最小, 这等价于在拟合能力与模型复杂度之间取得最优平衡。 (奥卡姆剃刀原理)
8. 多层:  多层神经网络,  并非多个模型的串级,  而是在一个模型内部,  多级函数复合.
    1. 模型串级的结构也有,  叫做「级联系统」,  他的问题在于,  训练的模型是模块化的,  并非端到端学习,  误差信号无法跨模型反向传播.
9. 特征
    1. 在传统机器学习中, 除了模型和学习算法外, 特征或表示也是影响最终学习效果的重要因素, 甚至在很多的任务上比算法更重要因此, 要开发一个实际的机器学习系统, 人们往往需要花费大量的精力去尝试设计不同的特征以及特征组合, 来提高最终的系统能力, 这就是所谓的特征工程问题.  (有符号主义的色彩)
        1. 表示学习,  是把需要手动构造的特征工程问题,  通过某种算法给自动化执行掉.
    2. 但即使这样, 人工设计的特征在很多任务上也不能满足需要. 因此, 如何让机器自动地学习出有效的特征也成为机器学习中的一项重要研究内容, 称为特征学习, 也叫表示学习. 特征学习在一定程度上也可以减少模型复杂性、缩短训练时间、提高模型泛化能力、避免过拟合等.
    3. 从某种角度来讲, 深度学习使得机器学习中的“特征工程”问题转变为“网络架构工程” 问题.
10. 从数据中抓取特征的方式叫做「表示」,  自动地把输入信息转换为有效的特征  (从而使用它进行机器学习) 的学习,  就是表示学习. 特征是人为规定的,  抓取特征的手段也是人为选取的. 通常有两种手段:  ①**局部表示**,  ②**分布式表示**. 前者使用 one-hot 向量,  即向量中只有某一个元素为1,  其余都是零. 不同特征对应不同的 one-hot 向量,  任意的 one-hot 向量都互斥, 因此用 one-hot 向量表示特征时,  这种表示方法无法说明特征之间的关系.
11. 局部表示
    1. 假设所有颜色的名字构成一个词表 v,  词表大小为|v|. 我们可以用1个|v|维的 one-hot 向量来表示每一种颜色. 在第 i 种颜色对应的 one-hot 向量中,  第 i 维的值为1,  其他都为0.
    2. ①可解释性好,  ②多种特征向量组合得到的矩阵是稀疏的二值矩阵. 局部表示适合人工总结和归纳,  通过特征组合进行特征工程.
    3. ①如果词表很大,  则 one-hot 向量的维度很高; ②不能扩展维度,  每增加一种特征的实例,  就需要增加一个维度.③无法知道特征实例之间的相似性.
    4. 用低纬度的 n 维稠密向量,  来表示一种特征的实例,  这是分布式表示. 例如使用 RGB 三个维度的不同数值来表示一种颜色. 不同的颜色对应 RGB 三维空间中的不同的点.
        1. 将高纬度的局部表示空间 $\mathcal{R}^{|v|}$ 映射到低纬度的分布表示空间 $\mathcal{R}^{D}$,  这个过程叫**嵌入**  (embedding).
        2. 低纬度的分布表示空间表示的特征下面的不同实例;
        3. 表示学习,  就是从底层特征开始  (某种 onehot 分类) 通过某种映射  (映射是要学的东西),  最后转变为低纬度的一般性分布式表示.
        4. 分布式表示时使用的特征, 可以是抽象的. 这正是深度学习强大的地方。虽然在 RGB 的例子中, R、G、B 都有明确的物理意义 (对应视网膜的三种锥状细胞), 但在机器学习 (特别是 NLP 或图像的 hidden layers) 中: 
            1. 特征是“学”出来的, 不是人定义的:  我们不再手动定义“R、G、B”轴, 而是告诉神经网络: “我给你一个维度为 128 的向量空间, 你去自己寻找最能区分这些数据的 128 个特征轴。”
            2. 特征通常是不可解释的 (Abstract/Latent):  如果你去查看一个训练好的词向量 (Word Embedding) 的第 37 维, 它可能代表某种无法用语言描述的概念。只要它有助于降低预测误差 (Loss)。
            3. 状态空间的状态向量,  可以表示 `[位置,  速度,  加速度]`,  这里三个分量都是特征,  具有明确的物理含义,  系统当前状态  (机器学习中的一个实例) 是这三种数值共同决定的一个向量.
            4. 但是在卡尔曼滤波或其他观测器设计中,  我们会引入中间状态变量,  他们在物理上可能找不到直接对应的传感器读数,  但是在数据上构成了描述系统的基  (basis).
    5. 从 onehot 表示到分布式表示,  本质上表示了从符号主义  (特征=个例),  到连接主义的变化  (个例=特征的组合). 我们将原本孤立的实例, 拆解成了更基础的基向量 (Basis Vectors) 的线性或非线性组合。
        1. One-hot: 特征 = 身份 ID。不同 ID 之间正交, 无信息共享。
        2. 分布式: 特征 = 属性/因子。不同实例通过共享这些因子的不同权重 (激活程度) 来定义自己。这些因子既可以是具象的 (如 RGB), 也可以是神经网络自动学习出的高度抽象概念。
12. 深度学习,  是深度的机器学习 (即深度学习 $\in$ 机器学习). 深度学习可以用来解决什么问题? 表示学习问题,  推理,  决策.
    1. 深度学习主要以神经网络模型为基础. 神经网络天然不是深度学习, 但深度学习天然是神经网络. 深度学习和神经网络之间的关系就像「货币和金银」
    2. 「深度」所谓“深度”是指原始数据进行非线性特征转换的次数. 深度也可以看作从输入节点到输出节点所经过的最长路径的长度. 
    3. 只要是超过一层的神经网络都会存在贡献度分配问题, 因此可以将超过一层的神经网络都看作深度学习模型. 深度学习采用的模型主要是神经网络模型, 其主要原因是神经网络模型可以使用误差反向传播算法, 从而可以比较好地解决贡献度分配问题. 
13. 模块学习和端到端学习
    1. 传统机器学习,  把一个任务的输入和输出之间人为地切割成很多的子模块,  缺点是每个模块都需要单独优化,  第二个问题是局部最优并不一定保证总体目标最优.
    2. 于是有端到端的训练或者说学习. 在学习过程中,  不区分模块或者阶段,  直接优化任务的总体目标. 训练的数据就是输入-输出对的形式,  没有其他额外信息.